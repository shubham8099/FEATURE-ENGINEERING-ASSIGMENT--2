{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fde04de-b8b7-40f1-8439-736c6b4c2731",
   "metadata": {},
   "source": [
    "# Jai maa saraswati :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a1440-b20a-4dfc-b842-6c31891e162a",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a205c4b-3f29-498a-9292-9054be7cb190",
   "metadata": {},
   "source": [
    "# Filter Method :\n",
    "\n",
    "* Filter method in feature selection is a technique where features are selected Based on their Statistical Properties \n",
    "  with respect to their Target Variable . It Does not involve training a specific machine learning model : Instead it \n",
    "  relies on statistical Measures to Rank or Score of Each Features .\n",
    "  \n",
    "# How Does it Work :\n",
    "\n",
    "* Feature Ranking / Scoring : Each feature is Evaluated independently of the other Based on certain Statistical measures\n",
    "  Like - correlation , mutual informtaion , chi-Sqaure , ANNOVA F-Value . These Measure assess the Relationship between \n",
    "  the Feature and the Target Variable .\n",
    "  \n",
    "* Selection Criteria : Feature are Ranked Based on the Above Measures . The Higher Score or Rank , the more Revalent\n",
    "  The Feature is Considered to be Predicting the Target Variable . \n",
    "  \n",
    "* Feature Selection : After Ranking or Scoring a fixed number of top-ranked Feature are Selected or a Threshold score is\n",
    "  Applied to select the most Revalent Feature . sometimes Feature below a certain threshold are Discarded .\n",
    "  \n",
    "* Independence of Features : Importantly , the Filter Method Does not consider Interactions between Features . Each Features\n",
    "  is assessed Based on their Relationship with Target Variable . \n",
    "  \n",
    "* Computational Efficiency : One of the Advantages of the Filter Method is it computational efficiency , as it typically \n",
    "  requires evaluating each Features only onces based on simple Statistical Measures ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361f2ef-6e6d-482b-888a-04054d93248e",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a0a3f-ce5b-481c-a9ed-34ded95d8b30",
   "metadata": {},
   "source": [
    "# FILTER METHOD :\n",
    "\n",
    "* The Filter Method Evaluates the Relevance of features based on statistical measures Independency of any machine Learning \n",
    "  Algorithum .\n",
    "  \n",
    "# How It is Work :\n",
    "\n",
    "* Features are Selected or ranked according to certain criteria or statistical Test . For Example we might use correlation\n",
    "  mutual information , chi-square Test  or another Statistical Test to assess the relevance of features .\n",
    "  \n",
    "# Examples :\n",
    "\n",
    "* Chi-Square Test \n",
    "* Correlation Coefficient\n",
    "* Mutual Information \n",
    "\n",
    "# Wrapper Method :\n",
    "\n",
    "* The Wrapper Method evaluates the subsets of Features by actually training and validation machine learning model .\n",
    "\n",
    "# How it Work :\n",
    "\n",
    "* Different subsets of features are tested by training a model and assessing the Performance(Example :- accuracy , F1 Score)\n",
    "  the process iterates to find the subset of the Feature and give the best performnace according to choosing Metrics .\n",
    "  \n",
    "# Examples :\n",
    "\n",
    "* Forward Selection: Starts with no features and adds them one by one, evaluating performance each time.\n",
    "* Backward Elimination: Starts with all features and removes them one by one based on performance.\n",
    "* Recursive Feature Elimination (RFE): Trains the model, ranks features based on their importance, and removes the least       important features iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d56c3f-55c0-47e5-a975-c0cca197b85d",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd545e-acf8-4479-9c81-2922f7bec4c8",
   "metadata": {},
   "source": [
    "# Embedded Feature Selection Methods :-\n",
    "\n",
    "* Embedded feature selection methods are a class of technique that incorporate feature selcetion as a part of model training\n",
    "  process . They differ from the filter methods , which select the featyres independently of any model , and wrapper method\n",
    "  which use a specific predictive model to evaluate feature subsets . \n",
    "  \n",
    "# Lasso Regression(L1 Regularization) :\n",
    "\n",
    "* Technique : Lasso(Least Absolute Shrinking and Selection Operation) is a regression method that include L1 regularization \n",
    "  The L1 penalty term in the objective function encourages sparsity in the coefficient estimates, leading to some             coefficients being exactly zero.\n",
    "  \n",
    "* Feature Selection : Features with Non-Zero coefficients are selected , while those with zero coefficient are Excluded from\n",
    "  the Model .\n",
    "  \n",
    "# Ridge Regression(L2 Regularization) : \n",
    "\n",
    "* Technique : Technique: Ridge regression adds an L2 penalty term to the regression objective function. Although it doesn't   inherently perform feature selection by setting coefficients to zero, it can be used in conjunction with other methods for   feature importance ranking.\n",
    "\n",
    "* Feature Selection : By analyzing feature coefficients in ridge regression, we can gauge the importance of features,         although L2 regularization alone does not eliminate features.\n",
    "\n",
    "# Elastic Net :\n",
    "\n",
    "* Technique : Elastic net Combine the L1 & L2 regularization Penalties . It is Particularly useful when there is correlation \n",
    "  among Features .\n",
    "  \n",
    "* Feature Selection : It can perfrom the features selection by Shrinking some coefficient to zero (via L1 Penalty) and \n",
    "  Shrinking others (via L2 Penalty), thus allowing for more balanced feature selection process . \n",
    "  \n",
    "# Regularized Logistic Regression :\n",
    "\n",
    "* Technique : Similar to Lasso Regression , regularized logistic regression includes L2 and L2 penalties in the Logistic \n",
    "  Regression Model .\n",
    "  \n",
    "* Feature Selection : L1 Regularization can lead to sparse models where some feature coefficient are zero , effectively \n",
    "  performing feature selection .\n",
    "  \n",
    "# Sparse PCA (Principal Component Analysis):\n",
    "\n",
    "* Technique: Sparse PCA is an adaptation of PCA that incorporates sparsity constraints to produce principal components that   are sparse.\n",
    "\n",
    "* Feature Selection: By analyzing the sparse components, we can identify which features are most important for explaining     the variance in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59ee52-3f56-4747-849a-246e00348bee",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb002a3c-b1b2-4824-bf37-ed8d8b346623",
   "metadata": {},
   "source": [
    "# Disadvantages of Filter Methods :\n",
    "\n",
    "# Independence from Model Perfromance : \n",
    "\n",
    "* Limitation : Filter method Evaluate features independently of the model . They Do not consider how features interact with\n",
    "  Each other or how they influence the model's performance .\n",
    "  \n",
    "* Impact : This can Result in the selection of features that might be Statistically significant but necessarily useful for \n",
    "  improving the model accuracy or other performance Metrics .\n",
    "  \n",
    "# Lack of Interaction Consideration :\n",
    "\n",
    "* Limitation : Filter methods typically assess each features importance individually . without Interaction between the         Features .\n",
    "\n",
    "* Impact : Important interaction between features be Missed , Leading to the selection of features that do not capture \n",
    "  the full Complexity of Data .\n",
    "  \n",
    "# Dependence on Statistcal Measures :  \n",
    "\n",
    "* Limitation : Filter method rely on statistical Measure(Example :- Correlation , Chi-Square test) to evaluate Feature\n",
    "  Importance .\n",
    "  \n",
    "* Impact : These measures may not always align with practical importance of features for the specific predictive model ,\n",
    "  Leading to the selection of features that do not improve the model perfromance . \n",
    "  \n",
    "# Assumption of Features Independence :\n",
    "\n",
    "* Limitation : Many Filter methods assume features are independent of each other .\n",
    "* Impact : In pratice , features often have complex dependencies . Methods that do not account for these independencies may\n",
    "  Make incorrect assumptions about feature Relevance . \n",
    "  \n",
    "# Inability To Capture Nonlinear Relationships :\n",
    "\n",
    "* Limitation : Filter methods Typically use Linear Statistical measures to assess feature relevance .\n",
    "* Impact : They may not effectively capture nonlinear relationships between features and the target variable , potentially\n",
    "  missing importance features . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297be984-7794-4a6c-9c17-a4354983db4d",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcaf24-dd60-43f2-bd82-99953dee2fc4",
   "metadata": {},
   "source": [
    "# High Dimensionality\n",
    "\n",
    "* Situation : You we Dataset Very large dataset with a large number of Features(High Dimensionality)\n",
    "* Reason : The filter method is well-suited fir high-Dimensional Datasets because it evalueates features independently of\n",
    "           The Machine Learning models . This make is less Computationally expensive compared to the Wapper Method , which\n",
    "           Requires training and Evaluating the model for each Features . \n",
    "* Example :  In a text classification problem with thousands of words (features), the Filter method can quickly assess the                relevance of individual words based on statistical measures such as mutual information or chi-square tests,                  without needing to train a model repeatedly .           \n",
    "           \n",
    "# Limited Computational Resource \n",
    "\n",
    "* Reason : The Filter method is Less computationally intensive Because it does not involve training the model for each                  Features . Instead it relies on Statistical tests or metrics to evaluate the Features Importance, which is\n",
    "           Computationally Efficient . \n",
    "* Example : In cases where you have limited hardware or time constraints, such as working on a local machine with limited               processing power, using the Filter method allows you to perform feature selection without extensive model                   training .\n",
    "\n",
    "# Preliminary Features Selection \n",
    "\n",
    "* Reason : The Filter method can be used as a First step to Quickly Reduce the number of Features beafore applying more\n",
    "           Complex and Computationally expensive methods like wrapper method . It help in the Eliminating irrelevant \n",
    "           or less Informative features in the Early Process .\n",
    "           \n",
    "* Example :  Before applying a complex ensemble method or neural network, you might use the Filter method to reduce the                  feature set to a manageable size based on statistical measures, streamlining subsequent modeling steps.         \n",
    "\n",
    "# Model Agnostic Approach \n",
    "\n",
    "* Reason : The Filter method does not Depend on any Specific machine Learning model . It uses statistical properties to \n",
    "           assess features making it versatile for different types of models. This is useful when we are uncertain which\n",
    "           model will be used for final Predictions .\n",
    "           \n",
    "* Example : In a data exploration phase where you haven't yet decided on a specific predictive model, you can use the Filter             method to identify important features without tying the selection process to any particular algorithm .          \n",
    "# Simplicity and Interpretability \n",
    "\n",
    "* Reason : The filter method i straightforward statistical approach makes it eaiser to interpret and understand which\n",
    "           Features are considerd important . This can be Valuable for domains where features interpretability is crucial .\n",
    "           \n",
    "* Example : In domains Like healthcare and Finanace , Where understanding the rationale behind feature is important for                 transparency and compliance, the Filter method provides clear insights based on statistical tests .         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91f2b4-90d0-410f-b8ac-7baf15251c73",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0c352-8095-404f-9a05-5f0baa80bbf6",
   "metadata": {},
   "source": [
    "# Undertsand Given Dataset :\n",
    "\n",
    "* By Identify and Categorize the Features(eg :- Demographic information , usage Statistics , Customer Service Interaction)\n",
    "  Understanding the Data Helps in assessing which Features is help to Predicting Churn .\n",
    "  \n",
    "# Define the Target Variable :\n",
    "\n",
    "* Clearly Define the Target Variable , which in this Telecom company Case whether a customer is chrun or not . This binary\n",
    "  Outcome will Focus for filtering Features . \n",
    "  \n",
    "# Choose Evaluation Metrics :\n",
    " \n",
    "* Decide on metrics to evalute the relevance of Features .  \n",
    "* Correlation Coefficient\n",
    "* Chi-Square Test\n",
    "* Mutual information \n",
    "\n",
    "# Apply Features Selection Methods :\n",
    "\n",
    "* Correlation Analysis: Calculate the correlation coefficients between each numerical feature and the target variable.         Features with higher absolute correlation values are more relevant. For categorical features, you may need to encode them   or use other methods.\n",
    "\n",
    "* Chi-Square Test: For categorical features, compute the Chi-Square statistic to assess the association between each feature   and the target variable. Features with higher Chi-Square values are considered more relevant.\n",
    "\n",
    "* Mutual Information: Calculate mutual information scores for both categorical and continuous features. Features with higher scores indicate a stronger relationship with the target variable\n",
    "\n",
    "# Rank Features :\n",
    "\n",
    "* Based on the Evaluation metrics, rank the Features Like :- We Might Rank them by correlation coefficient or Chi-Square\n",
    "  Statistics .\n",
    "  \n",
    "# Select the Top Features :\n",
    "\n",
    "*  Choose the top features based on their rankings. We can set a threshold (e.g., top 10 features) or use a percentile          approach (e.g., top 20% of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d05670-650a-4824-8bd9-64f1d02ddca1",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c254280-1dbd-4682-90d3-9f02d4c5662e",
   "metadata": {},
   "source": [
    "# Embedded Method :\n",
    "\n",
    "* The Embedded for Features Selection involves incorporating Features selection directly into the model Training Process       This method leverages the model's learning algorithm to automatically identify and select the most relevant features.\n",
    "\n",
    "# Understanding the Dataset \n",
    "\n",
    "* Before applying any methods, thoroughly understand your dataset. Identify all features, such as player statistics (goals,   assists, defensive metrics), team rankings, historical match outcomes, and any other relevant data.\n",
    "\n",
    "# Choose the Suitable Model :\n",
    "\n",
    "* Tree-based models (e.g., Decision Trees, Random Forests, Gradient Boosting Machines): These models assess feature           importance based on how well features split the data.\n",
    "\n",
    "* Regularized models (e.g., Lasso Regression, Ridge Regression, Elastic Net): Regularization techniques penalize the           complexity of the model, which can shrink the coefficients of less important features towards zero\n",
    "\n",
    "#  Prepare the Data\n",
    "\n",
    "* Preprocessing: Clean and preprocess your data. Handle missing values, normalize numerical features if necessary, and         encode categorical variables.\n",
    "\n",
    "* Split Data: Divide your dataset into training and testing sets to evaluate model performance. A common split is 70%         training and 30% testing\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "* For Tree-based Models: Train models like Random Forest or Gradient Boosting. These models naturally compute feature         importance scores during training. Feature importance is typically measured by metrics such as Gini importance (in Random   Forest) or gain (in Gradient Boosting).\n",
    "\n",
    "* For Regularized Models: Train models such as Lasso Regression or Elastic Net. These models apply regularization penalties   that can drive the coefficients of less important features to zero, effectively performing feature selection\n",
    "\n",
    "#  Assess Feature Importance\n",
    "\n",
    "* Tree-based Models: After training, retrieve feature importance scores. For example, in a Random Forest, you can access       feature importance via the feature_importances_ attribute.\n",
    "\n",
    "* Regularized Models: Examine the model coefficients. Features with non-zero coefficients are considered important, while     those with zero or very small coefficients are less relevant.\n",
    "\n",
    "# Validate the Model\n",
    "\n",
    "* Re-train with Selected Features: Re-train the model using only the selected features to ensure that the reduced feature     set still performs well.\n",
    "\n",
    "* Evaluate Performance: Assess the performance of the model on the test set using metrics such as accuracy, precision,         recall, or F1 score. Compare the performance with the model trained on the full feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e79f9-9091-43cd-ad7f-bff160ccc915",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1bae5-eea2-43e7-be0d-7292ba7f3751",
   "metadata": {},
   "source": [
    "# Wrapper Method :\n",
    "\n",
    "* The Wrapper Method for feature selection involves evaluating different subsets of features by training and assessing the     performance of the predictive model using these subsets. This method is computationally intensive but can be very           effective in identifying the optimal set of features for your model. Here’s how you would use the Wrapper Method to select   the best set of features for predicting house prices .\n",
    "\n",
    "# Understanding the Data :\n",
    "\n",
    "* Size: Square footage, number of rooms, etc.\n",
    "* Location: ZIP code, neighborhood, proximity to amenities.\n",
    "* Age: Year built, age of the property\n",
    "\n",
    "# Choose The Model :\n",
    "\n",
    "* Linear Regression\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* Gradient Boosting Machines\n",
    "\n",
    "# Define a Perfromance Metrics :\n",
    "\n",
    "* Mean Absolute Error (MAE)\n",
    "* Mean Squared Error (MSE)\n",
    "* Root Mean Squared Error (RMSE)\n",
    "* R-squared\n",
    "\n",
    "# Select a Feature Selections Strategy :\n",
    "\n",
    "* Forward Selection: Start with no features and iteratively add the feature that improves the model’s performance the most     until no significant improvement is observed.\n",
    "\n",
    "* Backward Elimination: Start with all features and iteratively remove the least important feature (the one whose removal     has the least impact on model performance) until no further improvement is observed.\n",
    "\n",
    "* Recursive Feature Elimination (RFE): Train the model, remove the least important feature, and repeat the process until the   desired number of features is reached\n",
    "\n",
    "# Evaluate and Select the Best Subset\n",
    "\n",
    "* Cross-Validation: Use cross-validation to assess the performance of each subset of features. This ensures that the           evaluation is robust and not overly fitted to a particular training set.\n",
    "\n",
    "* Choose the Optimal Set: Select the subset of features that yields the best performance metric according to your chosen       criteria (e.g., lowest MAE or highest R-squared).\n",
    "\n",
    "# Validate the Final Model\n",
    "\n",
    "* Train on Full Training Data: Once you have selected the best feature subset, train the final model using the full training   dataset.\n",
    "\n",
    "* Test on Validation Set: Evaluate the final model on a separate validation or test set to ensure that it generalizes well     to unseen data . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21869c9-aa99-44d1-8beb-c08e1c5eec17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
